{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77124d-f974-41ae-b4bd-888bee6b0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ccc4c-b03c-43a7-99cb-90f677b5cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the plots bigger\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2315f-abc8-4026-8ee8-cf7b9eea18ba",
   "metadata": {},
   "source": [
    "# Line detection\n",
    "\n",
    "Let's look at one last function from OpenCV that I think could prove useful in trying to fit lines on the image. This time it's actually an line detection algorithm!        \n",
    "\n",
    "Naturally, there is an already made OpenCV Tutorial on it that you should read, see [Hough Lines](https://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html).\n",
    "\n",
    "And, naturally, I will give you a notebook that describes all the issues that are not neccessarily obvious from the OpenCV tutorial. \n",
    "\n",
    "The basic idea of the algorithm is that instead of facing off the hard task of detecting extended objects, we reduce the problem to one of finding local peaks. To perform this reduction we transform our image pixel coordinates into a space in which it is easier to find those pixels that lie on a line (i.e. are colinear).         \n",
    "\n",
    "With Hough transform (read as \"Hoff\") we convert all pixels that are bright enough, i.e. are above a threshold value, into polar coordinates. In polar coordinates each pixel effectively becomes a sine or a cosine function of $\\theta$. The pixels that lie on the same line will have their cosine function intersect at some $(r_c, \\theta_c)$ point.             \n",
    "Converting $(r_c, \\theta_c)$ back to Cartesian coordinates gives us two points $(x_0, y_0)$ and $(x_c, y_c)$ which define the line that connects them. The more points lie on the same line, the more times lines will fall on that intersection. What we want then, is to set up a voting scheme that votes for all the $(r, \\theta)$ that bright pixels pass through, once converted to polar coordinates, and then find the peaks in that space. This is how we reduce the problem of looking for extended objects, to looking for local maxima.\n",
    "\n",
    "This works amazingly well, but it also suffers from a bit of a catch-22. Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed0d46-afad-4e26-8c87-08e229d72bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"images/hough_lines1.png\", cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img)\n",
    "\n",
    "lines = cv2.HoughLines(\n",
    "    image=img,\n",
    "    rho=1,\n",
    "    theta=np.pi/180, \n",
    "    threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71895764-f014-44fe-9ea1-bcfe90b6d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines(lines, image, nlines=-1, color=(255, 255, 255), lw=2):\n",
    "    \"\"\"\n",
    "    Draws Hough lines on a given image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lines : `cv2.HoughLines`\n",
    "        2D array of line parameters, line parameters are stored in [0][x] as\n",
    "        tuples.\n",
    "    image : `np.array`\n",
    "        Image on which to draw the lines\n",
    "    nlines : `int` or `slice`\n",
    "        Number of top-voted for lines to draw, or a slice object corectly \n",
    "        indexing the lines array. By default draws all lines.\n",
    "    color : `tuple`\n",
    "        Tuple of values defining a BGR color.\n",
    "    lw : `int`\n",
    "        Line width.\n",
    "    \"\"\"\n",
    "    dimx, dimy = image.shape\n",
    "    # convert to color image so that you can see the lines\n",
    "    draw_im = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    if isinstance(nlines, slice):\n",
    "        draw = lines[nlines]\n",
    "    else:\n",
    "        draw = lines[:nlines]\n",
    "    \n",
    "    for params in draw:\n",
    "        if len(params[0]) == 2:\n",
    "            # this is non-probabilistic hough branch\n",
    "            rho, theta = params[0]\n",
    "            x0 = np.cos(theta) * rho\n",
    "            y0 = np.sin(theta) * rho\n",
    "            pt1 = (\n",
    "                int(x0 - (dimx + dimy) * np.sin(theta)),\n",
    "                int(y0 + (dimx + dimy) * np.cos(theta))\n",
    "            )\n",
    "            pt2 = (\n",
    "                int(x0 + (dimx + dimy) * np.sin(theta)),\n",
    "                int(y0 - (dimx + dimy) * np.cos(theta))\n",
    "            )\n",
    "            cv2.line(draw_im, pt1, pt2, color, lw)\n",
    "        else:\n",
    "            # this is probabilistic hough branch\n",
    "            cv2.line(draw_im, (params[0, 0], params[0,1]), (params[0, 2], params[0, 3]), color, lw)\n",
    "        \n",
    "    return draw_im\n",
    "\n",
    "limg = np.zeros(img.shape, dtype=np.uint8)\n",
    "limg = draw_lines(lines, limg, 250)\n",
    "plt.imshow(limg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40caf379-fa65-403b-8585-edbbda72418c",
   "metadata": {},
   "source": [
    "Oh no, what happend! \n",
    "\n",
    "Predictably, because Hough Transform votes for all pixels that are not zero it also votes for the circles. To be more precise, disks. But the diameters of disks also are lines. Therefore - we get lines fitted to the circles as well.        \n",
    "Not just that, additionally we can see that for each object we seemingly fit many lines. This is not unexpected either, since we can consider each line in the image as a rectangle with the smallest possible diameter of at least 1 - then we have at least 3 lines per rectangle. One is the upper side, one is the lower side and one follows the diagonal. And also everything in between these extremes. \n",
    "\n",
    "So, unfortunately, using line detectors require us to clean our original image again. We can do that by thresholding, or edge detection, and then again clean up the retrieved lines so that we might find the best matching line. \n",
    "\n",
    "But even then, if we found a line, we in principle would not be sure how long that line would be. There is a so-called probabilistic Hough line detection algorithm that could return us the best guess of the line lengths, before we go on with some example strategies on how to clean up the image let's take a peek at what those lines look like.         \n",
    "As a side note, probabilistic Hough will likely perform much better in the example image, given that it's essentially a bitmap with crystal clear edges.\n",
    "\n",
    "Do note that the parameters for the probabilistic transform had to be adjusted for this particular image because otherwise too many lines would be returned (as in, all non-black pixels would be declared lines of length 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c702a4e-cd5e-42de-bb24-169f959370e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = cv2.HoughLinesP(\n",
    "    image=img,\n",
    "    rho=1,\n",
    "    theta=np.pi/360., \n",
    "    threshold=100,\n",
    "    minLineLength=10,\n",
    ")\n",
    "\n",
    "limg = np.zeros(img.shape, dtype=np.uint8)\n",
    "limg = draw_lines(lines, limg, slice(0, -1, 5))\n",
    "plt.imshow(limg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36382dee-f699-4ac7-9ef8-a842d76a5786",
   "metadata": {},
   "source": [
    "Ok, with the basic invocation out of the way, what can we do about helping Hough transform detect the correct trails?\n",
    "\n",
    "Thresholding won't help us in this case, because we already effectively have a premade bitmap. It could come in handy on real images though.\n",
    "\n",
    "Edge detectors would at least thin out our objects which would clean up the number of possible lines by a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfec511-c87d-4b70-b42c-61d2bae3b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"images/hough_lines1.png\", cv2.IMREAD_GRAYSCALE)\n",
    "edges = cv2.Canny(img, 100, 200)\n",
    "plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a825994-297a-49df-b4d0-61270640b53a",
   "metadata": {},
   "source": [
    "We can also take advantage of the threshold operator to weed out lines that don't hit a lot of pixels.       \n",
    "We do have to figure out a reasonable threshold for that however, how about 30% of the maximum image size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ff8fc-f524-422e-8c2f-822148f69d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimx, dimy = img.shape\n",
    "diagonal = np.sqrt(dimx**2 + dimy**2)\n",
    "threshold = int(0.3 * diagonal)\n",
    "print(f\"Threshold: {threshold} pixels\")\n",
    "\n",
    "lines = cv2.HoughLines(edges, 1, np.pi/180, threshold)\n",
    "\n",
    "limg = np.zeros(img.shape, dtype=np.uint8)\n",
    "limg = draw_lines(lines, limg)\n",
    "plt.imshow(limg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ccdfc-7253-4b2c-9017-bbe8ee8f6259",
   "metadata": {},
   "source": [
    "Wow, much better. Unfortunately, if we remember the extra-section in notebook `4_edge_detection` we can remember that the edge detectors are not be best when it comes to noise, and our results after applying them on an real image were not the best looking thing we made so far.                 \n",
    "It does show us that the Hough transform is really susceptible to filled in objects, but is really good at ignoring edge maps. To make this point really obvious, let's take a look at the same image as above, but one that does not use filled in circles from start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a424f-3f04-43ec-81f6-f8ca130870d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread(\"images/hough_lines2.png\", cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a090b1f-ae33-4c7d-8e06-55d3a5c26658",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = cv2.HoughLines(img2, 1, np.pi/180, threshold)\n",
    "\n",
    "limg = np.zeros(img2.shape, dtype=np.uint8)\n",
    "limg = draw_lines(lines, limg)\n",
    "plt.imshow(limg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d643e-7d47-4615-a21b-3f4e2cd998cb",
   "metadata": {},
   "source": [
    "Not as good as processing with Canny and guessing all of the required thresholding and canny parameters just right, but pretty good! Compared to our initial attempt at just naively running Hough transform on an unprocessed bitmap image it's more than half way there already.\n",
    "\n",
    "Obviously this only works because the whole image is as if we took a shortcut over Canny edge detection. So the question is, why even mention this? \n",
    "\n",
    "## Analysing real data revisited.\n",
    "\n",
    "The reasoning is that in astronomy we can rely on source catalogs. These are large catalogs of of all detected objects (stars, galaxies, asteroids etc.) and their parameters, such as position, color, size, brightness etc. This is additional external information we can fold into our processing to ensure we delete as much additional confusing signal there is without harming the trails!        \n",
    "But why even bother with doing all that when we could have gotten all the examples we did so far to work without doing that anyhow. \n",
    "\n",
    "To make this as clear as possible we need to revisit an real-data example we already saw in an earlier notebook (see `4_edge_detection`).             \n",
    "Let's repeat some steps again. For that we need to dump code we used before into the cells below. Hopefully, by now, a lot of that code is less mystical than it was. So let's start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf189a-f2ae-4e28-bdd3-a32013866e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.visualization as aviz\n",
    "import astropy.io.fits as fitsio\n",
    "\n",
    "# have you figured out what this is yet?\n",
    "# a hint is in the name\n",
    "def z_score_trim(image, nstd, set_to_limits=None):\n",
    "    mean = image.mean()\n",
    "    std = image.std()\n",
    "    \n",
    "    upperz = mean + nstd*std\n",
    "    lowerz = mean - nstd*std\n",
    "    \n",
    "    if set_to_limits is None:\n",
    "        upper = upperz\n",
    "        lower = lowerz\n",
    "    else:\n",
    "        upper, lower = set_to_limits\n",
    "        \n",
    "    image[image > upperz] = upper\n",
    "    image[image < lowerz] = lower\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Did you figure how this works too?\n",
    "def keep_top_percent(image, keep_percent):\n",
    "    hist, bars = np.histogram(image, \"auto\", density=True)\n",
    "    cdf = np.cumsum(hist*np.diff(bars))\n",
    "    cutoff = np.where(cdf > 1-keep_percent)[0][0]\n",
    "    image[image < bars[cutoff]] = 0\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868e3b6-88ae-4077-84c1-c2d2a8dcd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdulist = fitsio.open(\"images/bi327715.fits\")\n",
    "img = hdulist[0].data\n",
    "\n",
    "# Make a copy so we can refer back to original image if we need to\n",
    "processed_image = img.copy()\n",
    "\n",
    "processed_image = z_score_trim(processed_image, 5, (0, 0))\n",
    "processed_image = z_score_trim(processed_image, 3)\n",
    "processed_image = keep_top_percent(processed_image, 0.05)\n",
    "\n",
    "# Before we re-cast the values in the processed_image it's \n",
    "# always good to standardize them and then shift them so they are\n",
    "# bigger than zero, otherwise CV will kind of eat our image.\n",
    "# This part is not some statistical tool, it's a workaround over CV.\n",
    "processed_image = (processed_image-processed_image.mean())/processed_image.std()\n",
    "processed_image -= processed_image.min()\n",
    "processed_image = cv2.normalize(processed_image, processed_image, 0, 255, cv2.NORM_MINMAX)\n",
    "processed_image = cv2.convertScaleAbs(processed_image)\n",
    "\n",
    "threshold, thresholded_image = cv2.threshold(processed_image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "edges = cv2.Canny(thresholded_image, 200, 230)\n",
    "\n",
    "plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e351b-cfd1-4028-8fc4-835e8fe48eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimx, dimy = processed_image.shape\n",
    "diagonal = np.sqrt(dimx**2 + dimy**2)\n",
    "threshold = int(0.1 * diagonal)\n",
    "print(f\"Threshold: {threshold} pixels\")\n",
    "\n",
    "lines = cv2.HoughLines(edges, 1, np.pi/180, threshold)\n",
    "print(f\"Found {len(lines)} lines.\")\n",
    "\n",
    "limg = np.zeros(img.shape, dtype=np.uint8)\n",
    "limg = draw_lines(lines, limg)\n",
    "plt.imshow(limg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410e530-0e51-491c-a000-30e81589d10f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Ok, not bad, not great either. How are we supposed to tell good lines from bad ones in all that mess? Well, that's the challenge.\n",
    "\n",
    "Couple of ideas we already saw were:\n",
    "* Lines are expected to be very bright in our images -> good outlier rejection gets us really close already.\n",
    "* Good outlier rejection depends on the types of objects in the image, large bright objects raise the signal mean and std which makes making a good cut on data harder --> getting rid of known objects would help\n",
    "  * Getting rid of known objects would also have the effect of helping Hough line finding algorithm, as we demonstrated in this notebook\n",
    "  * To really drive home the point how much removing known sources would help, I've taken the liberty to make two really large plots of our original image and our image after outlier rejection and brightness cut.         \n",
    "    Nearly all of the points in the edge image (above) are actually real objects and not a bad detection by Canny algorithm.\n",
    "* Perhaps the solution to the problem does not lie in removing known objects. Even without having a catalog of known objects in our images available to us we could still rely on contours in order to describe and filter the shapes of objects.                        \n",
    "  Could we somehow find our lines by relying on contours instead?\n",
    "* Perhaps the real solution lies in a combination of these various approaches, where we use results from one approach to validate and confirm the results from another?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126054ed-3ca0-44c3-aeb7-07bdb60ce941",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "from matplotlib import animation\n",
    "\n",
    "\n",
    "img = cv2.imread(\"images/bi327715.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "images = [img[::-1], processed_image]\n",
    "im = ax.imshow(images[0])\n",
    "\n",
    "def init():\n",
    "    im.set_data(images[0])\n",
    "    return im,\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(images[i%2])\n",
    "    return im,\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=1000, interval=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507a8eb-eb7c-4511-a3be-c4c83263e2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
