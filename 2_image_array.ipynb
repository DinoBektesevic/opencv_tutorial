{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d17777b-ed9f-47c1-b2df-327e2ac8a8f8",
   "metadata": {},
   "source": [
    "# Basic image manipulation\n",
    "\n",
    "In this notebook we will showcase how we can use the array indexing and array math to perform basic image manipulation using OpenCV.\n",
    "\n",
    "We can start by importing required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a94a0-f6af-4fc3-83fb-e81dcc27f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32751a-2121-4d4f-b094-322229f9d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the plots bigger\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2a01c-dd50-4190-84b0-8199a8c4c668",
   "metadata": {},
   "source": [
    "We can then define some functions we will use very often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f002718-c731-4108-87a6-b563772f45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(imgpath, mode=cv2.IMREAD_GRAYSCALE):\n",
    "    \"\"\"\n",
    "    Open an image as an numpy array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    imgpath : `str`\n",
    "        Path to the image.\n",
    "    mode : `int`, optional\n",
    "        Mode with which to open the image, i.e. color vs black and white.\n",
    "        Must be one of the OpenCV recognized constants such as \n",
    "        `cv2.IMREAD_GRAYSCALE` or `cv2.IMREAD_COLOR`\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    img : `np.array`\n",
    "        Image.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    OSError - when the file could not be opened.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(imgpath, mode)\n",
    "    if img is None:\n",
    "        raise OSError(\"Can't open file:\", imgpath)\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def show(img, ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Show image using matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : `np.array`\n",
    "        Image to display.\n",
    "    ax : `matplotlib.pyplot.Axes` or `None`, optional\n",
    "        Ax on which to plot the image. If  no axis is given\n",
    "        a new figure is created.\n",
    "    kwargs : `dict`, optional\n",
    "        Keyword arguments that are passed to `imshow`.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "    ax.imshow(img, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6a683-5e22-470a-a48d-19eaf7d3cbb3",
   "metadata": {},
   "source": [
    "To break down what is happening here a bit - for those less familiar with Python syntax, we defined a could of functions: `open_image` and `show`.\n",
    "\n",
    "So far we wrote all our code in a cell in a notebook and then executed it. Writing flat code like that, code that executes exactly how you read it line by line, is referred to procedural coding. There's nothing necessarily wrong with doing that, but in cases when there is a lot of code to write and many little procedures to string together it often pays off to generalize them and organize them in small units. This helps us not repeat code over and over again. That way, when we want to change something later, we can change it in that one \"packaged\" up place and it will affect the whole codebase. In procedural coding this would not be the case - we would need to change the functionality every time it appears. \n",
    "\n",
    "Functions are these little packages of code that we can repeat multiple times. In python they are defined by using the keyword `def` which is followed by the function name. This means that the code that follows, belongs to this function and not the general code that is executed every time you run the cell, only when you **call** the function. To make the distinction of which code belongs to the function and which doesn't you need to indent it. \n",
    "\n",
    "The `function_name` is followed by parenthesis `()` and a colon `:`. The parenthesis usually contain the function arguments. Functions do not *have* to have arguments, but they usually do. Python will usually assign the values you give to function in the order you define them, f.e. for a `def function(arg1, arg2):` that is called as `function(1, 2)` Python will assume you meant`arg1=1` and `arg2=2`. These are called positional arguments. Sometimes, however, you might want to have some values that user is not expected to provide because there exist reasonable default values you can assume for them. In that case you can provide these values in the function declaration like `def function(arg1, arg2=2):`. We can also provide full argument names for our function as well, in which case we call them keyword arguments. For example, calling function `def function(arg1, arg2):` as `function(arg2=2, arg1=1)` would still assign `arg1=1` and `arg2=2` despite the reverse order we gave them in. Notice this is different than the example with positional arguments above.\n",
    "\n",
    "Following the function declaration is probably Python's best feature - documentation. We use the [NumPy Doc](https://numpydoc.readthedocs.io/en/latest/format.html) style documentation which means we have, usually, a one short line describing what the function does (not how it's implemented!), a section `Parameters` with a list of function arguments, their `types` and a short description and we usually have a `Returns` section describing the returned value and its `type`. Not all functions need to have all, or any, of these sections and some might warrant having additional ones, f.e. `Raises` when they raise an error.\n",
    "\n",
    "After the documentation follows the function definition, the bit of code that does something with the given input and, usually, that consist of some kind of a calculation of values based on the function input and we want to give that calculated value back to the user at the end - ergo we `return` the value(s) at the end of the function.\n",
    "\n",
    "So put all together:\n",
    "```\n",
    "def function_name(arg1, arg2):\n",
    "    \"\"\"Short description of function purpose.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arg1 : `type`\n",
    "        Short description of this argument\n",
    "    arg2 : `type`\n",
    "        Short description of this argument\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    val : `type`\n",
    "        Short description of the returned value(s)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    BTW, calling this function will also place a random order on Amazon\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    **VERY** useful to list some sometimes\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    Error - description of when this error is raised.\n",
    "    \"\"\"\n",
    "    # do something with the values you were given\n",
    "    result = arg1 + arg2\n",
    "    \n",
    "    return result\n",
    "```\n",
    "\n",
    "Let's get back on track now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6b89d-8d33-448a-91af-75b745111337",
   "metadata": {},
   "source": [
    "# How do real images look like\n",
    "\n",
    "Let's take a peek. We can inspect the array dimensions, data type and data value ranges.\n",
    "\n",
    "For examples we can use the images in the `images` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1a5b4-11e3-4589-a581-2d1a74d0b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgbw = open_image(\"images/cat.png\")\n",
    "show(imgbw, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bd7a7-8d62-4dab-bac3-37d744795249",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape     \", imgbw.shape)\n",
    "print(\"data type \", imgbw.dtype)\n",
    "print(\"max       \", imgbw.max())\n",
    "print(\"min       \", imgbw.min())\n",
    "print(\"mean      \", imgbw.mean())\n",
    "print(\"Image element: \", imgbw[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf6ded-c91b-4e9f-8c17-d02b915b5adc",
   "metadata": {},
   "source": [
    "Let's do the same, but let's just see what happens if we use a different image mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95dc651-c3fa-4b9e-90b0-cc099bb9c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgc = open_image(\"images/cat.png\", cv2.IMREAD_UNCHANGED)\n",
    "show(imgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe9b58-720a-4258-85ec-a0985bb7a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape     \", imgc.shape)\n",
    "print(\"data type \", imgc.dtype)\n",
    "print(\"max       \", imgc.max())\n",
    "print(\"min       \", imgc.min())\n",
    "print(\"mean      \", imgc.mean())\n",
    "print(\"Image element: \", imgc[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f06c1-c091-44ae-b48c-50c2b94d6ca4",
   "metadata": {},
   "source": [
    "Do you notice any differences between the two examples?          \n",
    "Is the image still black and white?   \n",
    "Can you spot differences between the way the two images look like?        \n",
    "Are our array elements the same?           \n",
    "Our dtypes?          \n",
    "\n",
    "Can we trust what our eyes see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58621ea7-5fe4-44f2-bdaa-4efc1a202ca4",
   "metadata": {},
   "source": [
    "## Doing image math\n",
    "\n",
    "Even basic operations on images, such as subtracting, can be complicated. At the same time they can be quite powerful. \n",
    "\n",
    "Let's start by applying the basic array math we just used in 1_numpy_arrays.       \n",
    "Adding a value to the whole array increases brightness of the image. Subtracting value does the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a909954-05de-4786-a0af-300fe0a1a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(imgc+90, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276ab95-f508-481d-8bb5-4876817760be",
   "metadata": {},
   "source": [
    "Slicing operators return image regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fada2-7790-4e8d-af34-81b9442861e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimx, dimy = imgbw.shape\n",
    "centerx, centery = int(dimx/2), int(dimy/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed010f-9a73-48ae-8814-559675801461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square selection\n",
    "box_size = 200\n",
    "cutout = imgbw[\n",
    "    centerx-box_size:centerx+box_size, \n",
    "    centery-box_size:centery+box_size\n",
    "]\n",
    "show(cutout, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9471-2d70-4075-9dfe-ab530826c69d",
   "metadata": {},
   "source": [
    "How would **you** use numpy indexing to select a circle?\n",
    "\n",
    "Note: scrolling down might spoil the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b4e68-a739-4b67-8394-2f25175ef2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0833505-8ae5-451e-96f3-3a16b9e1970d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744708ec-ec6e-41b9-bc43-bccff098a0a7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6593bde-9c14-4030-9162-c63b9634a069",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884e7e7-395c-4526-8da4-5e228e0f7896",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289deaf5-95c2-4dc0-9890-d55167e7e81d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a79c0-92da-4804-9008-b86186330a0f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3820e-e864-4ccb-8160-858f5f751324",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef432bc-e75d-40d3-b86e-2e9b94039567",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d596b-37cb-438e-9f6f-a01786570e92",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f0a8c-6a92-4448-b8ee-ea07f8191eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.ogrid[:dimx, :dimy]\n",
    "mask = ( (x-centerx)**2 + (y-centery)**2 ) > 200**2\n",
    "cutout = imgbw.copy()\n",
    "cutout[mask] = 0\n",
    "show(cutout, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e824839-09d0-4dc4-af89-20c0b3e565a6",
   "metadata": {},
   "source": [
    "OpenCv helps us a lot here by having a bunch of these methods already available which saves us a headache.\n",
    "\n",
    "**Note** - the way NumPy an OpenCV index arrays is not exactly the same. Above we could confidently state:\n",
    "```\n",
    "x, y = np.ogrid[:dimx, :dimy]\n",
    "```\n",
    "but with OpenCV the origin is not bottom left, it is the top left instead. \n",
    "```\n",
    "cv2.circle(mask, (centery, centerx), 200, (255, 255, 255), -1)\n",
    "```\n",
    "Using top left as origin is a well established convention in Computer Vision, so in this case it's `matplotlib` that's \"odd\". Naturally Matplotlib uses bottom left, because it's meant to be used as a plotting tool - not an image viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b05dc-e396-4e1f-9689-d92213bb3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(imgbw.shape, dtype=np.uint8)\n",
    "cv2.circle(mask, (centery, centerx), 200, 255, -1)\n",
    "cutout = cv2.bitwise_and(imgbw, mask) #or img = img & mask\n",
    "show(cutout, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128811d0-2412-44d2-af26-86b2e8fd96f4",
   "metadata": {},
   "source": [
    "Can you figure out what all the elements in the function mean?\n",
    "\n",
    "How about in the ellipse example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d786fe-be90-48a3-86a3-271ae1813089",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(imgbw.shape, dtype=np.uint8)\n",
    "sma, smi = 200, 150\n",
    "rot_angle = 0\n",
    "froma, toa = 0, 360\n",
    "cv2.ellipse(mask, (centery, centerx), (sma, smi), rot_angle, froma, toa, (255, 255, 255), -1)\n",
    "cutout = cv2.bitwise_and(imgbw, mask) #or img = img & mask\n",
    "show(cutout, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32468332-64d7-400b-9972-f74609f9ec0e",
   "metadata": {},
   "source": [
    "Note that we have a lot of power when drawing with `cv2`. We don't even have to worry about image boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a773344-bd65-44fc-98cc-d0cd3c26ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(imgbw.shape, dtype=np.uint8)\n",
    "sma, smi = 300, 400\n",
    "rot_angle = 45\n",
    "froma, toa = 270, 360\n",
    "cv2.ellipse(mask, (centery-200, centerx), (sma, smi), rot_angle, froma, toa, (255, 255, 255), -1)\n",
    "cutout = cv2.bitwise_and(imgbw, mask) #or img = img & mask\n",
    "show(cutout, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c4b5c-b300-4c9d-b6f9-22c8a651ccdf",
   "metadata": {},
   "source": [
    "## Doing some **useful** math to our images.\n",
    "\n",
    "Lets look at one example where we can \"improve\" our original image by applying just basic operations on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2dfb5-9de9-40bd-a67f-934d2892b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image(\"images/son1.png\")\n",
    "show(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4df9b-37d5-43b3-909d-23b46d55ea3d",
   "metadata": {},
   "source": [
    "Obviously, the image itself is not unreadable. However the picture of the sonnet is not the best.         \n",
    "There are obvious issues with the light gradient across the image that impedes our ability to read the text itself.\n",
    "\n",
    "I have taken a liberty of creating a synthetic background for this image that we can use to improve the readability of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc6ed8-06f1-4247-b4e0-80a8ba559b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = open_image(\"images/son2.png\")\n",
    "show(img2, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3adc4e4-53f5-4a71-8c94-2a326886b4d6",
   "metadata": {},
   "source": [
    "How would you use the second image to improve on the first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac0503-fd51-45c4-8cf1-bad448b6d02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0d324bc-0d65-461d-9957-719af08a9ea6",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b9683a-6a4d-458b-913b-2d5ef705db22",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ef12c-d37c-41cc-b7f6-9b15040246f7",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a6744-aea7-48de-bfc3-c694ed701be5",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfc68d-93a7-464f-a482-655f13879115",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39602d1-b96d-451d-b3ed-db49ad92a072",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8089c553-a40b-47ec-8ab4-ed9aabf99666",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8c3ff-4e14-426a-9be2-5cf6cf0cf429",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4920-78b1-4a0b-b4ab-4e176393a918",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab6c07-ebd3-4381-9f28-d0385ddab4a4",
   "metadata": {},
   "source": [
    "Ok, let's try subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721bda3-9684-42d5-a7d3-3f60862f7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = img - img2\n",
    "show(diff, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8abf6-4d01-4d34-ba3b-b25c5e915e20",
   "metadata": {},
   "source": [
    "Looks a little bit more uniform but what's with the sudden blotches?\n",
    "\n",
    "Annoyingly, the math on images doesn't always work as you would expect.          \n",
    "Let's take an example of two numpy arrays with a single element of type `uint8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1623c4-7a72-434a-ab67-e8fd18dfd1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1], dtype=np.uint8)\n",
    "b = np.array([2], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619f02f-4ae9-44bc-8768-e7815be27444",
   "metadata": {},
   "source": [
    "What do you think the following line will print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9716c3-b661-435f-916f-7989b4ae7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "a - b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6856c586-f14f-4f25-b46c-435be64cd781",
   "metadata": {},
   "source": [
    "Can you figure out why?\n",
    "\n",
    "What would you do to the first, or second image to improve on this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d5dc7-649c-445c-b07d-f6aad7472e48",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b232d-d853-47c8-8ea1-f10097f2b41c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7eb80-7467-4c2e-8091-267f16d12e89",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204dbd1-a905-43cf-8d4f-6ed7df618cf8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69237b-fa99-4876-b8c9-39d2a094fe38",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abe86f-d6b0-4c01-8d8c-5c8bdb03a9ad",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad5955-8f7c-4b42-9745-ce1a299aaf4e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2971f-d0f0-4f24-b98c-f77d6d36dc4e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7348386-709d-46ce-91a3-e92a440d713f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a79b59-5ed1-4bbf-95ed-40711ccb1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff[diff < 5] = 255\n",
    "show(diff, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635d6d8-fbce-4951-98c1-98dcf725d651",
   "metadata": {},
   "source": [
    "Is this the best we can do with subtraction? \n",
    "\n",
    "Is this the best we can do in general?      \n",
    "\n",
    "Think about it a bit, what is it we want? We know that, in real life, the paper is the same color top right and bottom left. It just appears on the image it's not. So we want the top right, where it's very bright \"white\", and bottom left, where it's dark \"black\" to become the same color.          I.e. we want to \"put in more white\" where it's black and no white where it's already white. What mathematical operation would normalize these different values to the same one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978d7294-9e4d-4630-82cb-06bb01bf93dd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1486b9d-6623-42d6-b52e-f42c5ec7c886",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ecfe89-8a17-492a-8cf3-026508ce328a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7a8e0-da88-4ea5-8c5b-66f36880af05",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231cc9af-2938-462a-bf7a-53a26f63b78f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113adeb7-1de7-4493-a73f-761005218afc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482be90a-54a4-4934-895f-87e2a3b536e2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7086bc-893f-49b4-90bb-55979638fc55",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3122a9-5815-481c-9d9f-49b830c27466",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562e6d56-a1dd-4e32-bee3-2d28278486ea",
   "metadata": {},
   "source": [
    "If you figured it out - great job! If not, what about division?\n",
    "\n",
    "On face value, dividing the images shouldn't suffer from the same overflow - so we won't need to manually add in data to fix it.\n",
    "\n",
    "Lets think about couple of extreme cases here:\n",
    "* What happens if we divide two very bright areas? \n",
    "* What happens with very dark areas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fd0d1-22b3-46b1-a85e-b97d708923f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "div = img/img2\n",
    "show(div, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ed21b-d69a-41f7-b51f-74b6e74a13a4",
   "metadata": {},
   "source": [
    "Let's check out some values on this image though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f1a84-d73b-4510-bf45-a7d932da242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max\", div.max())\n",
    "print(\"min\", div.min())\n",
    "print(\"avg\", div.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17f962-6943-4b51-ac80-a0f4682f8b86",
   "metadata": {},
   "source": [
    "So how come this image displays correctly?\n",
    "\n",
    "Once again, `cv2` has a lot of functionality to help us re-normalize our images, however, they might not always behave in the most intuitive ways because of the `uint8` overflow. A lot of the times, `matplotlib` and/or `cv2` will perform this step under the hood before displaying the image to us. This also means you can't always trust the things you see - because there's extra math that happens. \n",
    "\n",
    "Sometimes, however, `matplotlib` and `cv2` do get confused. In that case I usually manage to fix my woes by using the `convertScaleAbs` function from OpenCV. We can see it's documentation by running `help` built-in function or by relying on a bit of Jupyter Notebook magic and adding a question mark at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1dbce-b550-4ae3-b324-cde62d393b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.convertScaleAbs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ca33b-65e3-4f30-8f5a-99207f863c9f",
   "metadata": {},
   "source": [
    "Now an interesting question might be - what has this lesson actually taught us and will we have to do something similar to what we did with the poem to work on astronomical images? \n",
    "\n",
    "The answer is somewhat complicated. Partly - yes - what we did with the scan of Poem for Lena is kind of what happens with astronomical images. The particular step where we take away the effects of non-uniformly illuminated focal plane in particular is called flat-fielding. But unlike here, where I approximated the gradient in the image by slapping some gradients in numpy array, in a properly calibrated scientific instrument these effects are rigorously measured. \n",
    "\n",
    "What are all the effects that are removed before we even show you the image, and the procedures of addition/subtraction/divisions/multiplications/derivations etc. that are used to do that are all explained in great details in [Learn Astropy Guide to CCD Data Reduction](http://www.astropy.org/ccd-reduction-and-photometry-guide/v/dev/notebooks/01-00-Understanding-an-astronomical-CCD-image.html).            \n",
    "In the tutorials they will first artificially create all of the effects astronomical images suffer from, then simulate a perfect astronomical image without them, then add all of these effects into a fake realistic astronomical image. But that's not all! Then they will take that simulated realistic image and work the other way around to remove all the effects from it and end up with a well calibrated image.         \n",
    "I *very* much recommend **leafing** through the guide to get familiar with astronomical data. It is not required you repeat all the exercises and demonstrations within, but if you pay attention to the text and the examples - you will see they do pretty much a similar thing to what we did here.\n",
    "\n",
    "However, our hope is to already work with calibrated images. The calibration procedure have some generic steps, but they are also subject to the particulars of individual instrument, and we don't often build two of the same ones, meaning each one is a little bit different. So the answer is also partly - no. We will hopefully not have to calibrate our exposures as the AstroPy examples do. \n",
    "\n",
    "We will still work with data however. Data that we can make conscious purposeful quality cuts on. It is not difficult, therefore, to imagine that we might need to rescale our image (involves subtraction and division of the whole image), or, in a more targeted approach, I can imagine us wanting to select areas around bright stars in order to make clever local background estimation and subtraction on etc. We will see more of this in following notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ceb70-eb8c-48ee-8237-ca1a0ade923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example of how to create a simple gradient\n",
    "x, y = np.ogrid[:dimx, :dimy]\n",
    "mask = ( (x-centerx)**2 + (y-centery)**2 )\n",
    "show(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b66a0f-3ec2-4f58-9646-b1653ca607b6",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "* Matplotlib is a plotting utility that we can use to visually inspect our images\n",
    "   * pretty cool\n",
    "* OpenCV is a library that contains many many helpful functions to manipulate and visualize images\n",
    "* Images are represented as arrays of different dimensions       \n",
    "   * Black and white images are just arrays of numbers. The size of the array are the dimensions of the image, and the pixel values represent the pixel brightness in that point.\n",
    "   * Color images have 3 or 4 numbers for each of the pixel. The first 3 values represent the brightness of the pixel, but this time for the particular color they represent, and the 4th value is opacity/alpha/transparency.\n",
    "   * These values are usually only discrete integer values in some range. More precisely, for images most commonly shown on our screens, it's 8 bit unsigned integers in the [0, 255] range.\n",
    "* But this is not always the case        \n",
    "   * Scientific data often represents real measurement, so restricting it to 255 values represents a large loss of precision.\n",
    "   * To see our data we often have to squish the full data range into the available range to display the image on the screen. I.e we have to somehow \"bin\" the data into those 255 boxes. We'll see how we do this and what it means in the next exercise.\n",
    "* Basic math operations modify some colloquially understood image properties\n",
    "   * addition increases brightness\n",
    "   * subtraction decreases it\n",
    "   * we can remove trends and smooth over details in an image by selecting parts of image          \n",
    "     or by providing per-pixel values which we can add/subtract or divide out\n",
    "   * We can express these trends in images as functions over pixels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
